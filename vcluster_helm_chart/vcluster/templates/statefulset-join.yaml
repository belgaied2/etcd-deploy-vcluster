{{- if not .Values.headless }}
{{- $kind := include "vcluster.k3s.workloadKind" . -}}
apiVersion: apps/v1
kind: {{ $kind }}
metadata:
  name: {{ .Release.Name }}-join
  namespace: {{ .Release.Namespace }}
  labels:
    app: vcluster
    chart: "{{ .Chart.Name }}-{{ .Chart.Version }}"
    release: "{{ .Release.Name }}"
    heritage: "{{ .Release.Service }}"
{{- if .Values.labels }}
{{ toYaml .Values.labels | indent 4 }}
{{- end }}
{{- $annotations := merge .Values.annotations .Values.globalAnnotations }}
  {{- if $annotations }}
  annotations:
{{ toYaml $annotations | indent 4 }}
  {{- end }}
spec:
{{- if (eq $kind "StatefulSet") }}
  serviceName: {{ .Release.Name }}-headless
{{- end }}
  replicas: {{ sub .Values.replicas 1}}
{{- if (and (eq $kind "Deployment") (.Values.enableHA)) }}
  strategy:
    rollingUpdate:
      maxSurge: 1
    {{- if (eq (int .Values.replicas) 1) }}
      maxUnavailable: 0
    {{- else }}
      maxUnavailable: 1
    {{- end }}
    type: RollingUpdate
{{- end }}
  selector:
    matchLabels:
      app: vcluster
      release: {{ .Release.Name }}
  {{- if (eq $kind "StatefulSet") }}
  {{- if (hasKey .Values "volumeClaimTemplates") }}
  volumeClaimTemplates:
{{ toYaml .Values.volumeClaimTemplates | indent 4 }}
  {{- else if .Values.storage.persistence }}
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes: [ "ReadWriteOnce" ]
        {{- if .Values.storage.className }}
        storageClassName: {{ .Values.storage.className }}
        {{- end }}
        resources:
          requests:
            storage: {{ .Values.storage.size }}
  {{- end }}
  {{- end }}
  template:
    metadata:
  {{- if .Values.podAnnotations }}
      annotations:
{{ toYaml .Values.podAnnotations | indent 8 }}
  {{- end }}
      labels:
        app: vcluster
        release: {{ .Release.Name }}
      {{- range $k, $v := .Values.podLabels }}
        {{ $k }}: {{ $v | quote }}
      {{- end }}
    spec:
      terminationGracePeriodSeconds: 10
      nodeSelector:
{{ toYaml .Values.nodeSelector | indent 8 }}
      {{- if .Values.affinity }}
      affinity:
{{ toYaml .Values.affinity | indent 8 }}
      {{- else if .Values.enableHA }}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          # if possible avoid scheduling more than one pod on one node
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - vcluster
                - key: release
                  operator: In
                  values:
                  - {{ .Release.Name }}
              topologyKey: "kubernetes.io/hostname"
          # if possible avoid scheduling pod onto node that is in the same zone as one or more vcluster pods are running
          - weight: 50
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - vcluster
                - key: release
                  operator: In
                  values:
                  - {{ .Release.Name }}
              topologyKey: topology.kubernetes.io/zone
      {{- end }}
      tolerations:
{{ toYaml .Values.tolerations | indent 8 }}
      {{- if .Values.serviceAccount.name }}
      serviceAccountName: {{ .Values.serviceAccount.name }}
      {{- else }}
      serviceAccountName: vc-{{ .Release.Name }}
      {{- end }}
      volumes:
      {{- if or .Values.securityContext.runAsUser .Values.securityContext.runAsNonRoot }}
        - name: helm-cache
          emptyDir: {}
        - name: tmp
          emptyDir: {}
      {{- end }}
        - name: config
          emptyDir: {}
      {{- if .Values.volumes }}
{{ toYaml .Values.volumes | indent 8 }}
      {{- end }}
      {{- if .Values.coredns.enabled }}
        - name: coredns
          configMap:
            name: {{ .Release.Name }}-coredns
      {{- end }}
      {{- if not .Values.storage.persistence }}
        - name: data
          emptyDir: {}
      {{- end }}
      {{- if .Values.priorityClassName }}
      priorityClassName: {{ .Values.priorityClassName }}
      {{- end }}
      {{- if .Values.fsGroup }}
      securityContext:
        fsGroup: {{ .Values.fsGroup }}
      {{- end }}
      containers:
      {{- if not .Values.vcluster.disabled }}
      - image: {{ .Values.defaultImageRegistry }}{{ .Values.vcluster.image }}
        name: vcluster
        # k3s has a problem running as pid 1 and disabled agents on cgroupv2
        # nodes as it will try to evacuate the cgroups there. Starting k3s
        # through a shell makes it non pid 1 and prevents this from happening
        command:
          - /bin/sh
        args:
          - -c
          - {{ range $f := .Values.vcluster.command -}}
            {{ $f }}
            {{- end }}
          {{- range $f := .Values.vcluster.baseArgs }}
            {{ $f }}
          {{- end }}
          {{- if not .Values.sync.nodes.enableScheduler }}
            --server https://{{ .Release.Name }}.{{ .Release.Namespace }}:6443
            --advertise-address=$(POD_IP)
            --tls-san={{ .Release.Name }}.{{ .Release.Namespace }}.svc
            --disable-scheduler
            --kube-controller-manager-arg=controllers=*,-nodeipam,-nodelifecycle,-persistentvolume-binder,-attachdetach,-persistentvolume-expander,-cloud-node-lifecycle,-ttl
            --kube-apiserver-arg=endpoint-reconciler-type=none
          {{- else }}
            --kube-controller-manager-arg=controllers=*,-nodeipam,-persistentvolume-binder,-attachdetach,-persistentvolume-expander,-cloud-node-lifecycle,-ttl
            --kube-apiserver-arg=endpoint-reconciler-type=none
            --kube-controller-manager-arg=node-monitor-grace-period=1h
            --kube-controller-manager-arg=node-monitor-period=1h
          {{- end }}
          {{- if .Values.serviceCIDR }}
            --service-cidr={{ .Values.serviceCIDR }}
          {{- else }}
            --service-cidr=$(SERVICE_CIDR)
          {{- end }}
          {{- range $f := .Values.vcluster.extraArgs }}
            {{ $f }}
          {{- end }}
            && true
        env:
          {{- if .Values.vcluster.env }}
{{ toYaml .Values.vcluster.env | indent 10 }}
          {{- end }}
          - name: K3S_TOKEN
            valueFrom:
              secretKeyRef:
                name: {{ include "vcluster.k3s.tokenSecretName" . | quote }}
                key: {{ include "vcluster.k3s.serverTokenKey" . | quote }}
          {{- if not .Values.serviceCIDR }}
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: SERVICE_CIDR
            valueFrom:
              configMapKeyRef:
                name: "vc-cidr-{{ .Release.Name }}"
                key: cidr
          {{- end }}
        securityContext:
{{ toYaml .Values.securityContext | indent 10 }}
        volumeMounts:
          - name: config
            mountPath: /etc/rancher
{{ toYaml .Values.vcluster.volumeMounts | indent 10 }}
        resources:
{{ toYaml .Values.vcluster.resources | indent 10 }}
      {{- end }}
      {{- if not .Values.syncer.disabled }}
      {{- end }}
{{- end }} 